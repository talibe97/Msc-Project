{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c94c36",
   "metadata": {},
   "source": [
    "# 8 Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb46d8b",
   "metadata": {},
   "source": [
    "On here we should have several plots we will create to be able to present our analysis in such a way that it can be very well presented and appear nicely in our report and our \n",
    "\n",
    "### Chart Ideas so far\n",
    "    - rediction vs Actual curves\n",
    "\n",
    "    - Histogram of RUL errors\n",
    "\n",
    "    - Distribution of lifespans (already in preprocessing)\n",
    "\n",
    "    - Comparison bar chart (RMSE/MAE per model)\n",
    "\n",
    "    - Optional: Training/Validation loss over epochs for DL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da82a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.py (visualisation functions for RUL prediction)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def plot_actual_vs_predicted(y_true, y_pred, model_name=\"Model\", dataset_name=\"FD001\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_true, label=\"Actual RUL\", linestyle='dashed', marker='o')\n",
    "    plt.plot(y_pred, label=\"Predicted RUL\", linestyle='dashed', marker='x')\n",
    "    plt.title(f\"Actual vs Predicted RUL – {model_name} ({dataset_name})\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Remaining Useful Life\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_curve(history, model_name=\"Model\", dataset_name=\"FD001\"):\n",
    "    if not hasattr(history, \"history\"):\n",
    "        print(\"⚠️ Invalid history object.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(f\"Training vs Validation Loss – {model_name} ({dataset_name})\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss (MSE)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metric_comparison(metrics_list, dataset_name=\"FD001\"):\n",
    "    \"\"\"\n",
    "    metrics_list: list of dicts from evaluate_model(), e.g.,\n",
    "        [{\"model\": \"LSTM\", \"RMSE\": 15.2, \"MAE\": 11.3}, ...]\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(metrics_list)\n",
    "    df = df.set_index(\"model\")\n",
    "\n",
    "    ax = df.plot(kind=\"bar\", figsize=(10, 6))\n",
    "    plt.title(f\"RMSE / MAE Comparison – {dataset_name}\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_error_histogram(y_true, y_pred, model_name=\"Model\", dataset_name=\"FD001\"):\n",
    "    errors = y_pred - y_true\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(errors, bins=30, edgecolor='black')\n",
    "    plt.title(f\"Prediction Error Distribution – {model_name} ({dataset_name})\")\n",
    "    plt.xlabel(\"Prediction Error (Predicted - Actual)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_prediction_table(y_true, y_pred, model_name=\"Model\", n=10):\n",
    "    df = pd.DataFrame({\n",
    "        \"Actual RUL\": y_true,\n",
    "        \"Predicted RUL\": y_pred,\n",
    "        \"Error\": y_pred - y_true\n",
    "    })\n",
    "\n",
    "    df_sample = df.head(n).copy()\n",
    "    df_sample.index = [f\"Engine {i+1}\" for i in range(n)]\n",
    "    display(df_sample.round(2))\n",
    "\n",
    "\n",
    "# ✨ Additional Plot Ideas to Consider Later\n",
    "\n",
    "def plot_rul_boxplot(y_true, y_pred, model_name=\"Model\"):\n",
    "    errors = y_pred - y_true\n",
    "    df = pd.DataFrame({\"Errors\": errors, \"Type\": model_name})\n",
    "    sns.boxplot(data=df, x=\"Type\", y=\"Errors\")\n",
    "    plt.title(\"Boxplot of Prediction Errors\")\n",
    "    plt.ylabel(\"Prediction Error\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_rul_by_engine(y_true, y_pred, unit_ids=None, model_name=\"Model\"):\n",
    "    df = pd.DataFrame({\"Engine\": unit_ids, \"Actual\": y_true, \"Predicted\": y_pred})\n",
    "    df_melted = df.melt(id_vars=[\"Engine\"], value_vars=[\"Actual\", \"Predicted\"],\n",
    "                        var_name=\"Type\", value_name=\"RUL\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_melted, x=\"Engine\", y=\"RUL\", hue=\"Type\")\n",
    "    plt.title(f\"Predicted vs Actual RUL by Engine – {model_name}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_cross_dataset_heatmap(results_dict, metric='RMSE'):\n",
    "    \"\"\"\n",
    "    Visualises generalisation capability via heatmap.\n",
    "\n",
    "    Parameters:\n",
    "        results_dict (dict): \n",
    "            {\n",
    "                'FD001': {'FD001': 12.3, 'FD002': 20.1, ...},\n",
    "                'FD002': {'FD001': 15.0, 'FD002': 10.8, ...},\n",
    "                ...\n",
    "            }\n",
    "        metric (str): 'RMSE' or 'MAE' (for title only)\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results_dict)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(df, annot=True, fmt=\".1f\", cmap=\"YlGnBu\")\n",
    "    plt.title(f\"{metric} – Train vs Test Dataset\")\n",
    "    plt.xlabel(\"Test Dataset\")\n",
    "    plt.ylabel(\"Train Dataset\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14206c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cross_dataset_bars(results_dict, metric='RMSE'):\n",
    "    \"\"\"\n",
    "    Creates a grouped bar chart for train-test RMSE or MAE.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results_dict).T.reset_index().melt(id_vars='index', \n",
    "                      var_name='Test Set', value_name=metric)\n",
    "    df.rename(columns={'index': 'Train Set'}, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=df, x='Train Set', y=metric, hue='Test Set')\n",
    "    plt.title(f\"{metric} by Train/Test Combination\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
