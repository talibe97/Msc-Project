{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46811b60",
   "metadata": {},
   "source": [
    "# 4 Lstm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcb2ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from pre_processing import load_preprocessed_data\n",
    "from evaluator import evaluate_model\n",
    "\n",
    "\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    Builds and compiles an LSTM model for RUL prediction.\n",
    "\n",
    "    Parameters:\n",
    "        input_shape (tuple): Shape of input data (timesteps, features)\n",
    "\n",
    "    Returns:\n",
    "        model (keras.Model): Compiled LSTM model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_lstm_model(model, X_train, y_train, X_val, y_val, epochs=20, batch_size=64):\n",
    "    \"\"\"\n",
    "    Trains the LSTM model with training and validation data.\n",
    "\n",
    "    Parameters:\n",
    "        model (keras.Model): Compiled LSTM model\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        epochs (int): Number of epochs\n",
    "        batch_size (int): Batch size\n",
    "\n",
    "    Returns:\n",
    "        model: Trained Keras model\n",
    "        history: Training history object\n",
    "    \"\"\"\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def predict_lstm_model(model, X):\n",
    "    \"\"\"\n",
    "    Generates RUL predictions from the trained LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "        model (keras.Model): Trained LSTM model\n",
    "        X (np.ndarray): Input data to predict on\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Predicted RUL values\n",
    "    \"\"\"\n",
    "    return model.predict(X).flatten()\n",
    "\n",
    "def run_lstm_pipeline(npz_path=\"fd001_last.npz\", epochs=20, batch_size=64):\n",
    "    \"\"\"\n",
    "    Full pipeline to train, evaluate, and report LSTM model performance.\n",
    "\n",
    "    Parameters:\n",
    "        npz_path (str): Path to .npz file with preprocessed data\n",
    "        epochs (int): Number of training epochs\n",
    "        batch_size (int): Size of each training batch\n",
    "\n",
    "    Returns:\n",
    "        Tuple: model, y_val, y_pred, evaluation_results\n",
    "    \"\"\"\n",
    "    print(\"=== Step 1: Load Preprocessed Data ===\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_preprocessed_data(npz_path)\n",
    "    print(f\"Train shape: {X_train.shape}, {y_train.shape}\")\n",
    "    print(f\"Val shape  : {X_val.shape}, {y_val.shape}\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"=== Step 2: Build Model ===\")\n",
    "    input_shape = X_train.shape[1:]\n",
    "    model = build_lstm_model(input_shape)\n",
    "    model.summary()\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"=== Step 3: Train Model ===\")\n",
    "    model, history = train_lstm_model(model, X_train, y_train, X_val, y_val,\n",
    "                                      epochs=epochs, batch_size=batch_size)\n",
    "    print(\"Training complete.\\n\")\n",
    "\n",
    "    print(\"=== Step 4: Predict ===\")\n",
    "    y_pred = predict_lstm_model(model, X_val)\n",
    "    print(\"Sample predictions:\", y_pred[:5])\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"=== Step 5: Evaluate ===\")\n",
    "    evaluation_results = evaluate_model(y_val, y_pred, model_name=\"LSTM\")\n",
    "    print(\"\")\n",
    "\n",
    "    return model, y_val, y_pred, evaluation_results\n",
    "\n",
    "def save_lstm_model(model, filename=\"lstm_model.h5\"):\n",
    "    \"\"\"\n",
    "    Saves the trained LSTM model to an HDF5 (.h5) file.\n",
    "\n",
    "    Parameters:\n",
    "        model (keras.Model): Trained model to save\n",
    "        filename (str): Filename for saving\n",
    "    \"\"\"\n",
    "    model.save(filename)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "\n",
    "def load_lstm_model(filename=\"lstm_model.h5\"):\n",
    "    \"\"\"\n",
    "    Loads a saved LSTM model from an HDF5 (.h5) file.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): Filename of the saved model\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Loaded model\n",
    "    \"\"\"\n",
    "    from keras.models import load_model\n",
    "    model = load_model(filename)\n",
    "    print(f\"Model loaded from {filename}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f210754f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 30, 64)            20480     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 30, 64)            0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,929\n",
      "Trainable params: 32,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build_lstm_model testing \n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_preprocessed_data(\"fd001_last.npz\")\n",
    "\n",
    "# infer shape from training data\n",
    "input_shape = X_train.shape[1:]  # (timesteps, features)\n",
    "\n",
    "# build model\n",
    "model = build_lstm_model(input_shape)\n",
    "\n",
    "# summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e291c1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "222/222 [==============================] - 7s 22ms/step - loss: 7097.2627 - val_loss: 6296.0713\n",
      "Epoch 2/20\n",
      "222/222 [==============================] - 4s 20ms/step - loss: 5936.3330 - val_loss: 5380.7710\n",
      "Epoch 3/20\n",
      "222/222 [==============================] - 4s 20ms/step - loss: 5089.8066 - val_loss: 4608.9536\n",
      "Epoch 4/20\n",
      "222/222 [==============================] - 5s 20ms/step - loss: 4365.3662 - val_loss: 3947.1873\n",
      "Epoch 5/20\n",
      "222/222 [==============================] - 4s 20ms/step - loss: 3748.3806 - val_loss: 3376.8721\n",
      "Epoch 6/20\n",
      "222/222 [==============================] - 4s 20ms/step - loss: 3202.3462 - val_loss: 2881.4116\n",
      "Epoch 7/20\n",
      "222/222 [==============================] - 4s 20ms/step - loss: 2746.2454 - val_loss: 2457.7581\n",
      "Epoch 8/20\n",
      "222/222 [==============================] - 4s 20ms/step - loss: 2338.4438 - val_loss: 2092.7510\n",
      "Epoch 9/20\n",
      "222/222 [==============================] - 5s 20ms/step - loss: 1994.4727 - val_loss: 1772.4889\n",
      "Epoch 10/20\n",
      "222/222 [==============================] - 5s 20ms/step - loss: 1695.1680 - val_loss: 1497.8617\n",
      "Epoch 11/20\n",
      "222/222 [==============================] - 5s 20ms/step - loss: 1446.8811 - val_loss: 1269.0912\n",
      "Epoch 12/20\n",
      "222/222 [==============================] - 5s 23ms/step - loss: 1218.3334 - val_loss: 1076.0120\n",
      "Epoch 13/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 1037.2362 - val_loss: 913.6545\n",
      "Epoch 14/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 890.5344 - val_loss: 777.1828\n",
      "Epoch 15/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 766.0726 - val_loss: 660.8856\n",
      "Epoch 16/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 655.8141 - val_loss: 570.7153\n",
      "Epoch 17/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 567.1302 - val_loss: 491.0264\n",
      "Epoch 18/20\n",
      "222/222 [==============================] - 5s 22ms/step - loss: 505.0408 - val_loss: 422.7307\n",
      "Epoch 19/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 436.7564 - val_loss: 369.7421\n",
      "Epoch 20/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 388.5431 - val_loss: 362.3039\n"
     ]
    }
   ],
   "source": [
    "# train_lstm_model testing \n",
    "model, history = train_lstm_model(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "217ff83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 1s 5ms/step\n",
      "[ 77.676186  87.23597  104.237206 104.23795  102.93428 ]\n"
     ]
    }
   ],
   "source": [
    "# predict_lstm_model testing\n",
    "\n",
    "y_pred = predict_lstm_model(model, X_val)\n",
    "print(y_pred[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eff9725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Evaluation:\n",
      "  RMSE: 19.0343\n",
      "  MAE : 15.0567\n"
     ]
    }
   ],
   "source": [
    "# see how good my model is \n",
    "\n",
    "\n",
    "from evaluator import evaluate_model\n",
    "\n",
    "results = evaluate_model(y_val, y_pred, model_name=\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28a8dae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 1: Load Preprocessed Data ===\n",
      "Train shape: (14184, 30, 15), (14184,)\n",
      "Val shape  : (3547, 30, 15), (3547,)\n",
      "\n",
      "=== Step 2: Build Model ===\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 30, 64)            20480     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 30, 64)            0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,929\n",
      "Trainable params: 32,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "=== Step 3: Train Model ===\n",
      "Epoch 1/20\n",
      "222/222 [==============================] - 7s 23ms/step - loss: 6929.3384 - val_loss: 6123.5542\n",
      "Epoch 2/20\n",
      "222/222 [==============================] - 5s 20ms/step - loss: 5779.3223 - val_loss: 5231.9780\n",
      "Epoch 3/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 4949.4077 - val_loss: 4476.7310\n",
      "Epoch 4/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 4239.1753 - val_loss: 3827.9565\n",
      "Epoch 5/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 3632.4658 - val_loss: 3268.1099\n",
      "Epoch 6/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 3102.1079 - val_loss: 2787.0688\n",
      "Epoch 7/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 2653.0994 - val_loss: 2370.4856\n",
      "Epoch 8/20\n",
      "222/222 [==============================] - 5s 20ms/step - loss: 2258.3130 - val_loss: 2016.3640\n",
      "Epoch 9/20\n",
      "222/222 [==============================] - 4s 20ms/step - loss: 1935.0635 - val_loss: 1708.1770\n",
      "Epoch 10/20\n",
      "222/222 [==============================] - 4s 20ms/step - loss: 1631.4071 - val_loss: 1443.5101\n",
      "Epoch 11/20\n",
      "222/222 [==============================] - 5s 20ms/step - loss: 1392.6727 - val_loss: 1222.7535\n",
      "Epoch 12/20\n",
      "222/222 [==============================] - 4s 20ms/step - loss: 1190.2366 - val_loss: 1036.5721\n",
      "Epoch 13/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 1008.2728 - val_loss: 883.1853\n",
      "Epoch 14/20\n",
      "222/222 [==============================] - 4s 20ms/step - loss: 858.5958 - val_loss: 756.9919\n",
      "Epoch 15/20\n",
      "222/222 [==============================] - 5s 22ms/step - loss: 734.8583 - val_loss: 642.1027\n",
      "Epoch 16/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 631.3245 - val_loss: 548.7336\n",
      "Epoch 17/20\n",
      "222/222 [==============================] - 5s 22ms/step - loss: 544.0934 - val_loss: 473.8716\n",
      "Epoch 18/20\n",
      "222/222 [==============================] - 5s 20ms/step - loss: 484.0551 - val_loss: 416.2911\n",
      "Epoch 19/20\n",
      "222/222 [==============================] - 5s 21ms/step - loss: 427.7759 - val_loss: 355.9075\n",
      "Epoch 20/20\n",
      "222/222 [==============================] - 4s 20ms/step - loss: 378.7051 - val_loss: 313.7674\n",
      "Training complete.\n",
      "\n",
      "=== Step 4: Predict ===\n",
      "111/111 [==============================] - 1s 5ms/step\n",
      "Sample predictions: [ 72.6593  104.84982 104.93448 104.93771  87.24498]\n",
      "\n",
      "=== Step 5: Evaluate ===\n",
      "LSTM Evaluation:\n",
      "  RMSE: 17.7135\n",
      "  MAE : 14.1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, y_val, y_pred, results = run_lstm_pipeline(\"fd001_last.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80f5578b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to my_lstm_fd001.h5\n"
     ]
    }
   ],
   "source": [
    "save_lstm_model(model, \"my_lstm_fd001.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca41e136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from my_lstm_fd001.h5\n"
     ]
    }
   ],
   "source": [
    "model = load_lstm_model(\"my_lstm_fd001.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0868b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lstm_data(file_path, seq_len=30, save_as=\"fd001_last.npz\"):\n",
    "    \"\"\"\n",
    "    Prepares LSTM-ready 3D data from raw C-MAPSS file using pre_processing module.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to raw FD001 text file.\n",
    "        seq_len (int): Length of each time window.\n",
    "        save_as (str): Filename to save the .npz file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: X_train, y_train, X_val, y_val\n",
    "    \"\"\"\n",
    "    # Step 1: Load raw data\n",
    "    df = data_loader.load_raw_data(file_path)\n",
    "    \n",
    "    # Step 2: Drop flat sensors\n",
    "    df = pre_processing.drop_flat_sensors(df)\n",
    "    \n",
    "    # Step 3: Calculate RUL\n",
    "    df = pre_processing.calculate_rul(df)\n",
    "    \n",
    "    # Step 4: Standardise\n",
    "    df = pre_processing.standardise_per_condition(df)\n",
    "    \n",
    "    # Step 5: Generate sliding windows\n",
    "    X, y = pre_processing.generate_sliding_windows(df, seq_len=seq_len)\n",
    "    \n",
    "    # Step 6: Train/val split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Step 7: Save\n",
    "    pre_processing.save_preprocessed_data(X_train, y_train, X_val, y_val, X_test=None, y_test=None, filename=save_as)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
